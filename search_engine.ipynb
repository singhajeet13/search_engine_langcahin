{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0740356",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import ArxivQueryRun, WikipediaQueryRun\n",
    "from langchain_community.utilities import ArxivAPIWrapper, WikipediaAPIWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b08d9205",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inbuild Wikkipedia Tool\n",
    "api_wrapper_wiki = WikipediaAPIWrapper(\n",
    "    top_k_results=1,\n",
    "    doc_content_chars_max=250\n",
    ")\n",
    "\n",
    "tool_wiki = WikipediaQueryRun(\n",
    "    api_wrapper=api_wrapper_wiki,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f773440f",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_wrapper_arxiv = ArxivAPIWrapper(\n",
    "    top_k_results=1,\n",
    "    doc_content_chars_max=250\n",
    ")\n",
    "\n",
    "tool_arxiv = ArxivQueryRun(\n",
    "    api_wrapper=api_wrapper_arxiv,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ffb9d69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arxiv'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_arxiv.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6e9fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [tool_wiki, tool_arxiv]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8342663f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "## Custom tool - RAG\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import faiss\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "420a78c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com/\")\n",
    "docs = loader.load()\n",
    "documents = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200).split_documents(docs)\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\", model_kwargs={\"device\": \"cpu\", \"token\": \"UdApfqIAOCtpiBtzcmyqSgMpMmTlZMLTPS\"})\n",
    "vectorstore = faiss.FAISS.from_documents(documents, embeddings)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f3b7779c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001FD813C8260>, search_kwargs={})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "90c7448f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'langsmith-search'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriver_tool = create_retriever_tool(\n",
    "    retriever=retriever,\n",
    "    name=\"langsmith-search\",\n",
    "    description=\"Search anything related to LangSmith documentation. Use this tool when you need to find information in the LangSmith documentation.\",\n",
    ")\n",
    "\n",
    "retriver_tool.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "44c45958",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [tool_wiki, tool_arxiv, retriver_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6abdae21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WikipediaQueryRun(api_wrapper=WikipediaAPIWrapper(wiki_client=<module 'wikipedia' from 'c:\\\\Users\\\\ajeet_singh\\\\Documents\\\\Ajeet\\\\Projects\\\\search_engine_langchain\\\\search_engine_langcahin\\\\.venv\\\\Lib\\\\site-packages\\\\wikipedia\\\\__init__.py'>, top_k_results=1, lang='en', load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " ArxivQueryRun(api_wrapper=ArxivAPIWrapper(arxiv_search=<class 'arxiv.Search'>, arxiv_exceptions=(<class 'arxiv.ArxivError'>, <class 'arxiv.UnexpectedEmptyPageError'>, <class 'arxiv.HTTPError'>), top_k_results=1, ARXIV_MAX_QUERY_LENGTH=300, continue_on_failure=False, load_max_docs=100, load_all_available_meta=False, doc_content_chars_max=250)),\n",
       " Tool(name='langsmith-search', description='Search anything related to LangSmith documentation. Use this tool when you need to find information in the LangSmith documentation.', args_schema=<class 'langchain_core.tools.retriever.RetrieverInput'>, func=functools.partial(<function _get_relevant_documents at 0x000001FD81E4DE40>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001FD813C8260>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'), coroutine=functools.partial(<function _aget_relevant_documents at 0x000001FD81E4DF80>, retriever=VectorStoreRetriever(tags=['FAISS', 'HuggingFaceEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001FD813C8260>, search_kwargs={}), document_prompt=PromptTemplate(input_variables=['page_content'], input_types={}, partial_variables={}, template='{page_content}'), document_separator='\\n\\n', response_format='content'))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12ca9925",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RUN these tools with Agents and LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e6da4126",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4adaba35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"Llama3-8b-8192\",\n",
    "    api_key=groq_api_key,\n",
    "    max_tokens=1000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ea78a145",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='chat_history', optional=True),\n",
       " HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}),\n",
       " MessagesPlaceholder(variable_name='agent_scratchpad')]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## prompt template\n",
    "from langchain import hub\n",
    "prompt = hub.pull(\"hwchase17/openai-functions-agent\")\n",
    "prompt.messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3e42028b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Agents to integrate tools with LLM\n",
    "from langchain.agents import create_openai_tools_agent\n",
    "\n",
    "agent = create_openai_tools_agent(\n",
    "    llm = llm,\n",
    "    tools=tools,\n",
    "    prompt=prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6862eb04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableAssign(mapper={\n",
       "  agent_scratchpad: RunnableLambda(lambda x: format_to_openai_tool_messages(x['intermediate_steps']))\n",
       "})\n",
       "| ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001FD209FB420>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]], 'agent_scratchpad': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x000001FD209FB420>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, metadata={'lc_hub_owner': 'hwchase17', 'lc_hub_repo': 'openai-functions-agent', 'lc_hub_commit_hash': 'a1655024b06afbd95d17449f21316291e0726f13dcfaf990cc0d18087ad689a5'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], input_types={}, partial_variables={}, template='You are a helpful assistant'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], input_types={}, partial_variables={}, template='{input}'), additional_kwargs={}), MessagesPlaceholder(variable_name='agent_scratchpad')])\n",
       "| RunnableBinding(bound=ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001FD88EDF560>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001FD81FADA60>, model_name='Llama3-8b-8192', model_kwargs={}, groq_api_key=SecretStr('**********'), max_tokens=1000), kwargs={'tools': [{'type': 'function', 'function': {'name': 'wikipedia', 'description': 'A wrapper around Wikipedia. Useful for when you need to answer general questions about people, places, companies, facts, historical events, or other subjects. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'query to look up on wikipedia', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'arxiv', 'description': 'A wrapper around Arxiv.org Useful for when you need to answer questions about Physics, Mathematics, Computer Science, Quantitative Biology, Quantitative Finance, Statistics, Electrical Engineering, and Economics from scientific articles on arxiv.org. Input should be a search query.', 'parameters': {'properties': {'query': {'description': 'search query to look up', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}, {'type': 'function', 'function': {'name': 'langsmith-search', 'description': 'Search anything related to LangSmith documentation. Use this tool when you need to find information in the LangSmith documentation.', 'parameters': {'properties': {'query': {'description': 'query to look up in retriever', 'type': 'string'}}, 'required': ['query'], 'type': 'object'}}}]}, config={}, config_factories=[])\n",
       "| OpenAIToolsAgentOutputParser()"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2a931bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## run agent\n",
    "from langchain.agents import AgentExecutor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2180d0ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `langsmith-search` with `{'query': 'LangSmith'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[38;5;200m\u001b[1;3mGet started with LangSmith | ðŸ¦œï¸ðŸ› ï¸ LangSmith\n",
      "\n",
      "Skip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\n",
      "LangSmith is a platform for building production-grade LLM applications.\n",
      "It allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\n",
      "ObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic â€” score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\n",
      "\n",
      "LangSmith + LangChain OSSLangSmith is framework-agnostic â€”Â it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\n",
      "For more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\n",
      "Observabilityâ€‹\n",
      "Observability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\n",
      "This is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmithâ€™s observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\n",
      "\n",
      "Get started by adding tracing to your application.\n",
      "Create dashboards to view key metrics like RPS, error rates and costs.\n",
      "\n",
      "Evalsâ€‹\n",
      "The quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\n",
      "\n",
      "Get started by creating your first evaluation.\n",
      "Quickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\n",
      "Analyze results of evaluations in the LangSmith UI and compare results over time.\n",
      "Easily collect human feedback on your data to improve your application.\n",
      "\n",
      "Prompt Engineeringâ€‹\n",
      "While traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.\u001b[0m\u001b[32;1m\u001b[1;3mIt seems that LangSmith is a platform for building production-grade LLM (Large Language Model) applications. It provides tools for observability, evaluation, and prompt engineering to help developers create and optimize their applications.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is LangSmith?',\n",
       " 'output': 'It seems that LangSmith is a platform for building production-grade LLM (Large Language Model) applications. It provides tools for observability, evaluation, and prompt engineering to help developers create and optimize their applications.',\n",
       " 'intermediate_steps': [(ToolAgentAction(tool='langsmith-search', tool_input={'query': 'LangSmith'}, log=\"\\nInvoking: `langsmith-search` with `{'query': 'LangSmith'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'wbq1x5f7x', 'function': {'arguments': '{\"query\":\"LangSmith\"}', 'name': 'langsmith-search'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'Llama3-8b-8192', 'system_fingerprint': 'fp_5b339000ab', 'service_tier': 'on_demand'}, id='run--9a3f04bb-6376-4722-bf3e-ab1c19c2163d', tool_calls=[{'name': 'langsmith-search', 'args': {'query': 'LangSmith'}, 'id': 'wbq1x5f7x', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1167, 'output_tokens': 64, 'total_tokens': 1231}, tool_call_chunks=[{'name': 'langsmith-search', 'args': '{\"query\":\"LangSmith\"}', 'id': 'wbq1x5f7x', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='wbq1x5f7x'),\n",
       "   \"Get started with LangSmith | ðŸ¦œï¸ðŸ› ï¸ LangSmith\\n\\nSkip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referenceGet StartedOn this pageGet started with LangSmith\\nLangSmith is a platform for building production-grade LLM applications.\\nIt allows you to closely monitor and evaluate your application, so you can ship quickly and with confidence.\\nObservabilityAnalyze traces in LangSmith and configure metrics, dashboards, alerts based on these.EvalsEvaluate your application over production traffic â€” score application performance and get human feedback on your data.Prompt EngineeringIterate on prompts, with automatic version control and collaboration features.\\n\\nLangSmith + LangChain OSSLangSmith is framework-agnostic â€”\\xa0it can be used with or without LangChain's open source frameworks langchain and langgraph.If you are using either of these, you can enable LangSmith tracing with a single environment variable.\\nFor more see the how-to guide for setting up LangSmith with LangChain or setting up LangSmith with LangGraph.\\nObservability\\u200b\\nObservability is important for any software application, but especially so for LLM applications. LLMs are non-deterministic by nature, meaning they can produce unexpected results. This makes them trickier than normal to debug.\\nThis is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmithâ€™s observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production.\\n\\nGet started by adding tracing to your application.\\nCreate dashboards to view key metrics like RPS, error rates and costs.\\n\\nEvals\\u200b\\nThe quality and development speed of AI applications depends on high-quality evaluation datasets and metrics to test and optimize your applications on. The LangSmith SDK and UI make building and running high-quality evaluations easy.\\n\\nGet started by creating your first evaluation.\\nQuickly assess the performance of your application using our off-the-shelf evaluators as a starting point.\\nAnalyze results of evaluations in the LangSmith UI and compare results over time.\\nEasily collect human feedback on your data to improve your application.\\n\\nPrompt Engineering\\u200b\\nWhile traditional software applications are built by writing code, AI applications involve writing prompts to instruct the LLM on what to do. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.\")]}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is LangSmith?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a9ae2c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Transformer model'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Transformer (deep learning architecture)\n",
      "Summary: In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted i\u001b[0m\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `wikipedia` with `{'query': 'Transformer model architecture'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mPage: Transformer (deep learning architecture)\n",
      "Summary: In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted i\u001b[0m\u001b[32;1m\u001b[1;3mThe transformer model is a type of deep learning architecture that is based on the multi-head attention mechanism. It was introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017. The transformer model is particularly well-suited for tasks such as machine translation, text summarization, and natural language processing.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is transformer model?',\n",
       " 'output': 'The transformer model is a type of deep learning architecture that is based on the multi-head attention mechanism. It was introduced in the paper \"Attention Is All You Need\" by Vaswani et al. in 2017. The transformer model is particularly well-suited for tasks such as machine translation, text summarization, and natural language processing.',\n",
       " 'intermediate_steps': [(ToolAgentAction(tool='wikipedia', tool_input={'query': 'Transformer model'}, log=\"\\nInvoking: `wikipedia` with `{'query': 'Transformer model'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': 'nva1yqh8g', 'function': {'arguments': '{\"query\":\"Transformer model\"}', 'name': 'wikipedia'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'Llama3-8b-8192', 'system_fingerprint': 'fp_0fb809dba3', 'service_tier': 'on_demand'}, id='run--32f66e3a-80fc-4c16-833a-7ca3c35c447c', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'Transformer model'}, 'id': 'nva1yqh8g', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1167, 'output_tokens': 68, 'total_tokens': 1235}, tool_call_chunks=[{'name': 'wikipedia', 'args': '{\"query\":\"Transformer model\"}', 'id': 'nva1yqh8g', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='nva1yqh8g'),\n",
       "   'Page: Transformer (deep learning architecture)\\nSummary: In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted i'),\n",
       "  (ToolAgentAction(tool='wikipedia', tool_input={'query': 'Transformer model architecture'}, log=\"\\nInvoking: `wikipedia` with `{'query': 'Transformer model architecture'}`\\n\\n\\n\", message_log=[AIMessageChunk(content='', additional_kwargs={'tool_calls': [{'index': 0, 'id': '4yt7vx8db', 'function': {'arguments': '{\"query\":\"Transformer model architecture\"}', 'name': 'wikipedia'}, 'type': 'function'}]}, response_metadata={'finish_reason': 'tool_calls', 'model_name': 'Llama3-8b-8192', 'system_fingerprint': 'fp_0fb809dba3', 'service_tier': 'on_demand'}, id='run--d63ce509-f339-4e33-8048-409293de0929', tool_calls=[{'name': 'wikipedia', 'args': {'query': 'Transformer model architecture'}, 'id': '4yt7vx8db', 'type': 'tool_call'}], usage_metadata={'input_tokens': 1279, 'output_tokens': 35, 'total_tokens': 1314}, tool_call_chunks=[{'name': 'wikipedia', 'args': '{\"query\":\"Transformer model architecture\"}', 'id': '4yt7vx8db', 'index': 0, 'type': 'tool_call_chunk'}])], tool_call_id='4yt7vx8db'),\n",
       "   'Page: Transformer (deep learning architecture)\\nSummary: In deep learning, transformer is an architecture based on the multi-head attention mechanism, in which text is converted to numerical representations called tokens, and each token is converted i')]}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is transformer model?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8084e33c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Another way to make agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59a3ecd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import init_chat_model\n",
    "# from langchain_tavily import TavilySearch\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7e9f97f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = MemorySaver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64623814",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2 = init_chat_model(\"Llama3-8b-8192\", model_provider=\"groq\", max_tokens = 1000, api_key=groq_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7e8261e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "\n",
    "agent_executor = create_react_agent(model_2, tools, checkpointer=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ed83abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "be7cb7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_message = {\"role\": \"user\", \"content\": \"What is LangSmith? tell me pointwise adwantages. provide 5 points\"}\n",
    "response = agent_executor.invoke({\"messages\": [input_message]}, config=config)\n",
    "\n",
    "# input_message = {\"role\": \"user\", \"content\": \"What is langsmith?\"}\n",
    "# for step in agent_executor.stream(\n",
    "#     {\"messages\": [input_message]}, config, stream_mode=\"values\"\n",
    "# ):\n",
    "#     step[\"messages\"][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "18eba961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 points about LangSmith, including its advantages:\n",
      "\n",
      "**What is LangSmith?**\n",
      "LangSmith is a platform for building production-grade LLM (Large Language Model) applications.\n",
      "\n",
      "**Advantages:**\n",
      "\n",
      "1. **Observability**: LangSmith provides LLM-native observability, allowing developers to get meaningful insights from their applications. This helps with debugging and identifying issues.\n",
      "2. **Evaluation**: LangSmith offers high-quality evaluation datasets and metrics, making it easy to test and optimize LLM applications.\n",
      "3. **Prompt Engineering**: LangSmith provides tools to help developers find the perfect prompt for their application, which is essential for LLM applications.\n",
      "4. **Monitoring and Debugging**: LangSmith allows developers to monitor and debug their LLM applications, making it easier to identify and fix issues.\n",
      "5. **Faster Development and Deployment**: LangSmith's features and tools enable faster development and deployment of LLM applications, which is critical for staying competitive in the market.\n"
     ]
    }
   ],
   "source": [
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f38e72",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "search-engine-langcahin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
